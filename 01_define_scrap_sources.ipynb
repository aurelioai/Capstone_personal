{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">The purpose here is to define the source of university rankings data, then check how to scrap each data and then how to merge the data from different sources.</h3>\n",
    "<h4 style=\"color:blue;\">The plan now is to scrap data from three different sources. This would lead to three different webpages, with three different scrapping algorithms and finally the collection of data. One thing I would like to do is to run a similarity analysis between the sources to see how consistent are between them.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PROJECT TITLE</h1>\n",
    "<h2>Supplementary information (code development)</h2>\n",
    "<h5>By: Aurelio Álvarez Ibarra</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>S1.1 Getting information from the ranking tables</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, first we will download and import the necessary packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages and libraries ready\n",
    "!pip install beautifulsoup4 lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Source one: Times Higher Education</h4>\n",
    "The first source of data will be the ranking by <a href=\"https://www.timeshighereducation.com/world-university-rankings/2020/world-ranking\">Times Higher Education</a>. <b>SAY SOMETHING ABOUT THE SOURCE...</b>. The data will be restricted to some Latin American countries. The ones available in this ranking are:\n",
    "<ul>\n",
    "    <li>Argentina (identified as AR)</li>\n",
    "    <li>Brasil (identified as BR)</li>\n",
    "    <li>Chile (identified as CL)</li>\n",
    "    <li>Colombia (identified as CO)</li>\n",
    "    <li>Costa Rica (identified as CR)</li>\n",
    "    <li>Cuba (identified as CU)</li>\n",
    "    <li>Jamaica (identified as JM)</li>\n",
    "    <li>Mexico (identified as MX)</li>\n",
    "    <li>Peru (identified as PE)</li>\n",
    "    <li>Puerto Rico (identified as PR)</li>\n",
    "    <li>Venezuela (identified as VE)</li>\n",
    "</ul>\n",
    "Let's try this first source with the full list. In the process the structure of the webpage will be studied in order to systematically retrieve the data for every countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data from webpage\n",
    "myurl = 'https://www.timeshighereducation.com/world-university-rankings/2020/world-ranking#!/page/0/length/-1/locations/AR/sort_by/rank/sort_order/asc/cols/stats'\n",
    "## The structure of the webpage is:\n",
    "##   1.- Main URL and starting page (0):\n",
    "##       https://www.timeshighereducation.com/world-university-rankings/2020/world-ranking#!/page/0/\n",
    "##   2.- Number of results per page (-1 means \"All\"):\n",
    "##       length/-1/\n",
    "##   3.- Location definition (check the country identifier above):\n",
    "##       locations/AR/\n",
    "##   4.- Sorting parameters:\n",
    "##       sort_by/rank/sort_order/asc/\n",
    "##   5.- Information shown (\"stats\"=ranking data; \"scores\"=scoring data):\n",
    "##       cols/stats\n",
    "source = requests.get(myurl)\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the Times Higher Education server is to deny incoming requests from this type (403). Sometimes, webpages require to define a <code>User-Agent</code> as explained <a href=\"https://stackoverflow.com/questions/38489386/python-requests-403-forbidden\">here</a>. On top of that, when retrieving the table section of the webpage, the <code>tbody</code> tag (which encloses the data in the body of the table) is empty. According to this <a href=\"https://stackoverflow.com/questions/49260014/beautifulsoup-returns-empty-td-tags\">source</a>, the table data may be generated by a script and not with the HTML code itself. Thus, extra information must be included in the header and in the <code>requests.get</code> arguments to get the results of such script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data from webpage\n",
    "##AR_url = 'https://www.timeshighereducation.com/world-university-rankings/2020/world-ranking#!/page/0/length/-1/locations/AR/sort_by/rank/sort_order/asc/cols/stats'\n",
    "myurl = 'https://www.timeshighereducation.com/world-university-rankings/2020/world-ranking#!/page/0/length/-1/sort_by/rank/sort_order/asc/cols/stats'\n",
    "## old__header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36','Accept': 'application/json, text/javascript, */*; q=0.01'}\n",
    "source = requests.get(myurl,headers=header).text # Without .text, it gets the response in JSON format\n",
    "mysoup = BeautifulSoup(source,'lxml')\n",
    "mytable = mysoup.find('table')\n",
    "print(mytable.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous results in a table object with an emtpy <code>tbody</code> tag as well. Unfortunately, I could not figure out how to get the actual data from the table. I checked many sources but it would be too much for me to understand it right away. I tried with another retriever but got the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My try with urlib.requests... generates the same result (empty tbody)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "url_to_scrape = 'https://www.timeshighereducation.com/world-university-rankings/2020/world-ranking#!/page/0/length/-1/sort_by/rank/sort_order/asc/cols/stats'\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url_to_scrape).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decide to drop source one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Source two: U.S. News</h4>\n",
    "The second source of data will be the ranking by <a href=\"https://www.usnews.com/education/best-global-universities/rankings\">U.S. News</a>. <b>SAY SOMETHING ABOUT THE SOURCE...</b>. The data will be restricted to some Latin American countries. The ones available in this ranking are:\n",
    "<ul>\n",
    "    <li>Argentina</li>\n",
    "    <li>Brasil</li>\n",
    "    <li>Chile</li>\n",
    "    <li>Colombia</li>\n",
    "    <li>Costa Rica</li>\n",
    "    <li>Cuba</li>\n",
    "    <li>Jamaica</li>\n",
    "    <li>Mexico</li>\n",
    "    <li>Peru</li>\n",
    "    <li>Puerto Rico</li>\n",
    "    <li>Venezuela</li>\n",
    "</ul>\n",
    "Let's try this second source with one country. In the process of gathering this sample data, the structure of the webpages will be studied in order to systematically retrieve the data for all countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure of the page, the data is not in a table but in a set of <code>div</code> tags with <code>class=\"sep\"</code>. A concerning details is that it is not possible to ask for the full list in one shot, but this will be solved later. For the first page of results for the Latin American universities, the retrieving code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in the search result:  10\n",
      "<div class=\"sep\">\n",
      "<div class=\"thumb-right\">\n",
      "<div class=\"t-large t-strong t-constricted\">66.4</div>\n",
      "<div class=\"t-smaller t-dim\">Global Score</div>\n",
      "<div class=\"levelmeter\" style=\"background-position: -97.19999999999999px 0px; width:80px; height: 8px;\"></div>\n",
      "</div>\n",
      "<div class=\"thumb-left\">\n",
      "<span class=\"rankscore-bronze\">\n",
      "          #1\n",
      "        </span>\n",
      "</div>\n",
      "<div class=\"block unwrap\">\n",
      "<h2 class=\"h-taut\">\n",
      "<a href=\"https://www.usnews.com/education/best-global-universities/universidade-de-sao-paulo-500437\">Universidade de São Paulo</a>\n",
      "</h2>\n",
      "<div class=\"t-taut\">\n",
      "<i class=\"flag-22 flag-brazil-22\"></i>\n",
      "<span>Brazil</span>\n",
      "<span class=\"t-dim t-small\">São Paulo</span>\n",
      "</div>\n",
      "<div>\n",
      "<img alt=\"\" class=\"icon thumb-left-bleed\" src=\"/static/images/badges/micro-badge-silver-20.png\" srcset=\"/static/images/badges/micro-badge-silver-20.png, /static/images/badges/micro-badge-silver-40.png 2x\" width=\"20\"/>\n",
      "          #128 <span>(tied)</span> – <a data-ajax=\"true\" href=\"/education/best-global-universities/rankings\">Best Global Universities</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Save data from webpage\n",
    "myurl = 'https://www.usnews.com/education/best-global-universities/search?region=latin-america'\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36','Accept': 'application/json, text/javascript, */*; q=0.01'}\n",
    "source = requests.get(myurl,headers=header).text # Without .text, it gets the response in JSON format\n",
    "mysoup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "\n",
    "# I need this function to find the div with the specific class\n",
    "mydivs = mysoup.find_all(lambda tag: tag.name == 'div' and\n",
    "                         tag.get('class') == ['sep'])\n",
    "print(\"Elements in the search result: \",len(mydivs))\n",
    "print(mydivs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, I get a list as a result. According to its lenght, I know how many elements I shall analyze. For the first ten results in the Latin America ranking, the following data can be retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Universidade de São Paulo, #1 in Latin America with a global score of 66.4, is located in São Paulo (Brazil).\n",
      "** Pontificia University Católica de Chile, #2 in Latin America with a global score of 57.2, is located in Santiago (Chile).\n",
      "** State University of Campinas, #3 in Latin America with a global score of 56.7, is located in Campinas, São Paulo (Brazil).\n",
      "** Federal University of Rio de Janeiro, #4 in Latin America with a global score of 54.9, is located in Rio de Janeiro (Brazil).\n",
      "** University of Buenos Aires, #5 in Latin America with a global score of 53.7, is located in Buenos Aires City, Buenos Aires (Argentina).\n",
      "** National Autonomous University of Mexico, #6 in Latin America with a global score of 53.4, is located in Ciudad de México, Distrito Federal (Mexico).\n",
      "** University of Chile, #7 in Latin America with a global score of 53.1, is located in Santiago (Chile).\n",
      "** University of the Andes Colombia, #8 in Latin America with a global score of 51.4, is located in Bogotá, DC (Colombia).\n",
      "** Universidad Tecnica Federico Santa Maria, #9 in Latin America with a global score of 51.1, is located in Valparaiso, Valparaiso (Chile).\n",
      "** Federal University of Rio Grande do Sul, #10 in Latin America with a global score of 50.4, is located in Porto Alegre, Rio Grande do Sul (Brazil).\n"
     ]
    }
   ],
   "source": [
    "import re # For the regex search in the local ranking\n",
    "###\n",
    "ind = 0\n",
    "univnames = []\n",
    "for myUname in mysoup.find_all(\"h2\",class_=\"h-taut\"): # All tags with university name\n",
    "#    print(myUname.text.strip())\n",
    "    univnames.append(myUname.text.strip())\n",
    "    ind = ind+1\n",
    "###\n",
    "ind = 0\n",
    "LAranks = []\n",
    "for myLArank in mysoup.find_all(\"div\",class_=\"thumb-left\"): # All tags with rank in LatinAmerica (see myurl)\n",
    "    # Score comes with a # sign and maybe a TIE string. Convert to number\n",
    "    LAranks.append(int(re.findall(\"[0-9]+\",str(myLArank))[0]))\n",
    "    ind = ind+1\n",
    "###\n",
    "ind = 0\n",
    "globalscores = []\n",
    "for myGscore in mysoup.find_all(\"div\",class_=\"t-large t-strong t-constricted\"): # All tags with global score number\n",
    "    globalscores.append(myGscore.text)\n",
    "    ind = ind+1\n",
    "###\n",
    "ind = 0\n",
    "countries = []\n",
    "cities = []\n",
    "for mylocation in mysoup.find_all(\"div\",class_=\"t-taut\"): # All tags with location\n",
    "    countries.append(mylocation.span.text.strip())\n",
    "    cities.append(mylocation.find(\"span\",class_=\"t-dim t-small\").text.strip())\n",
    "    ind = ind+1\n",
    "###\n",
    "for ind in range(len(mydivs)):\n",
    "    print('** {}, #{} in Latin America with a global score of {}, is located in {} ({}).'.format(univnames[ind],LAranks[ind],globalscores[ind],cities[ind],countries[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next pages, an extra string appears in the URL: <code>page=#</code>, where <code>#</code> goes from 2 to 10 in this particular case. For <code>#</code>=1, this \"extra\" string does not appear. The following loop retrieves as many pages as stated in <code>numpages</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fields\n",
    "univnames = []\n",
    "LAranks = []\n",
    "globalscores = []\n",
    "countries = []\n",
    "cities = []\n",
    "pagenum = []\n",
    "###\n",
    "\n",
    "# Defining extra strings in URL and setting up request\n",
    "myurl = 'https://www.usnews.com/education/best-global-universities/latin-america'\n",
    "numpages = 10\n",
    "extrastring = ['']\n",
    "for mypages in range(2,numpages+1):\n",
    "    mytext = '?page='\n",
    "    extrastring.append(mytext+str(mypages))\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "          +' Chrome/64.0.3282.186 Safari/537.36','Accept': 'application/json, text/javascript, */*; q=0.01'}\n",
    "\n",
    "# Loop for pages 1 to numpages\n",
    "for n in range(1,numpages+1):\n",
    "    url = myurl + extrastring[n-1]\n",
    "    source = requests.get(url,headers=header).text\n",
    "    mysoup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "    ### University names\n",
    "    for myUname in mysoup.find_all(\"h2\",class_=\"h-taut\"): # All tags with university name\n",
    "        univnames.append(myUname.text.strip())\n",
    "    ### University rank (in Latin America)\n",
    "    for myLArank in mysoup.find_all(\"div\",class_=\"thumb-left\"): # All tags with rank in LatinAmerica (see myurl)\n",
    "        try:\n",
    "            LAranks.append(int(re.findall(\"[0-9]+\",str(myLArank))[0]))\n",
    "        except:\n",
    "            LAranks.append(None)\n",
    "    ### University global score\n",
    "    for myGscore in mysoup.find_all(\"div\",class_=\"t-large t-strong t-constricted\"): # All tags with global score number\n",
    "        globalscores.append(myGscore.text.strip())\n",
    "    ### University location (country and city)\n",
    "    for mylocation in mysoup.find_all(\"div\",class_=\"t-taut\"): # All tags with global score\n",
    "        countries.append(mylocation.span.text.strip())\n",
    "        cities.append(mylocation.find(\"span\",class_=\"t-dim t-small\").text.strip())\n",
    "    ### Page number of this batch of data\n",
    "    maxscores = len(mysoup.find_all(\"div\",class_=\"t-large t-strong t-constricted\"))\n",
    "    for mypage in range(1,maxscores+1):\n",
    "        pagenum.append(n)\n",
    "# Universities without global score do not have complete data (see below), thus they will be dropped\n",
    "##for ind in range(len(globalscores)):\n",
    "##    print('** {}, #{} in Latin America with a global score of {}, is located in {} ({}).'.format(univnames[ind],LAranks[ind],globalscores[ind],cities[ind],countries[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the lists in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University</th>\n",
       "      <th>LatinAmericaRank</th>\n",
       "      <th>GlobalScore</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>PageNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Universidade de São Paulo</td>\n",
       "      <td>1</td>\n",
       "      <td>66.4</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pontificia University Católica de Chile</td>\n",
       "      <td>2</td>\n",
       "      <td>57.2</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State University of Campinas</td>\n",
       "      <td>3</td>\n",
       "      <td>56.7</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Campinas, São Paulo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Federal University of Rio de Janeiro</td>\n",
       "      <td>4</td>\n",
       "      <td>54.9</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University of Buenos Aires</td>\n",
       "      <td>5</td>\n",
       "      <td>53.7</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires City, Buenos Aires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>National Autonomous University of Mexico</td>\n",
       "      <td>6</td>\n",
       "      <td>53.4</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Ciudad de México, Distrito Federal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>University of Chile</td>\n",
       "      <td>7</td>\n",
       "      <td>53.1</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>University of the Andes Colombia</td>\n",
       "      <td>8</td>\n",
       "      <td>51.4</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Bogotá, DC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Universidad Tecnica Federico Santa Maria</td>\n",
       "      <td>9</td>\n",
       "      <td>51.1</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Valparaiso, Valparaiso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Federal University of Rio Grande do Sul</td>\n",
       "      <td>10</td>\n",
       "      <td>50.4</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Porto Alegre, Rio Grande do Sul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 University  LatinAmericaRank GlobalScore  \\\n",
       "0                 Universidade de São Paulo                 1        66.4   \n",
       "1   Pontificia University Católica de Chile                 2        57.2   \n",
       "2              State University of Campinas                 3        56.7   \n",
       "3      Federal University of Rio de Janeiro                 4        54.9   \n",
       "4                University of Buenos Aires                 5        53.7   \n",
       "5  National Autonomous University of Mexico                 6        53.4   \n",
       "6                       University of Chile                 7        53.1   \n",
       "7          University of the Andes Colombia                 8        51.4   \n",
       "8  Universidad Tecnica Federico Santa Maria                 9        51.1   \n",
       "9   Federal University of Rio Grande do Sul                10        50.4   \n",
       "\n",
       "     Country                                City  PageNumber  \n",
       "0     Brazil                           São Paulo           1  \n",
       "1      Chile                            Santiago           1  \n",
       "2     Brazil                 Campinas, São Paulo           1  \n",
       "3     Brazil                      Rio de Janeiro           1  \n",
       "4  Argentina     Buenos Aires City, Buenos Aires           1  \n",
       "5     Mexico  Ciudad de México, Distrito Federal           1  \n",
       "6      Chile                            Santiago           1  \n",
       "7   Colombia                          Bogotá, DC           1  \n",
       "8      Chile              Valparaiso, Valparaiso           1  \n",
       "9     Brazil     Porto Alegre, Rio Grande do Sul           1  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "maxlenght = len(globalscores)\n",
    "USN_df=pd.DataFrame({'University':univnames[0:maxlenght],\n",
    "                        'LatinAmericaRank':LAranks[0:maxlenght],\n",
    "                        'GlobalScore':globalscores[0:maxlenght],\n",
    "                        'Country':countries[0:maxlenght],\n",
    "                        'City':cities[0:maxlenght],\n",
    "                        'PageNumber':pagenum[0:maxlenght]}\n",
    "                      )\n",
    "USN_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for the university ranking appear when clicking on the university's name. The resulting URL includes a variant of the university and a numeric identifier. This information can be found in an <code>a</code> tag inside the <code>h2</code> tag used to retriege the university name. The previous data retriever will be used to get the URL for each university."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University</th>\n",
       "      <th>LatinAmericaRank</th>\n",
       "      <th>GlobalScore</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>PageNumber</th>\n",
       "      <th>USN_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Universidade de São Paulo</td>\n",
       "      <td>1</td>\n",
       "      <td>66.4</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pontificia University Católica de Chile</td>\n",
       "      <td>2</td>\n",
       "      <td>57.2</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State University of Campinas</td>\n",
       "      <td>3</td>\n",
       "      <td>56.7</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Campinas, São Paulo</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Federal University of Rio de Janeiro</td>\n",
       "      <td>4</td>\n",
       "      <td>54.9</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University of Buenos Aires</td>\n",
       "      <td>5</td>\n",
       "      <td>53.7</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires City, Buenos Aires</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Universidade Tecnologica Federal do Parana</td>\n",
       "      <td>79</td>\n",
       "      <td>16.5</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Curitiba</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Universidade Federal de Sergipe</td>\n",
       "      <td>80</td>\n",
       "      <td>16.3</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>São Cristóvão</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Universidade Federal Rural de Pernambuco (UFRPE)</td>\n",
       "      <td>81</td>\n",
       "      <td>15.9</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Recife, PE</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Universidad Autonoma de Baja California</td>\n",
       "      <td>82</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexicali, Baja California</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Universidad Veracruzana</td>\n",
       "      <td>82</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Xalapa, Veracruz</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.usnews.com/education/best-global-u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          University  LatinAmericaRank  \\\n",
       "0                          Universidade de São Paulo                 1   \n",
       "1            Pontificia University Católica de Chile                 2   \n",
       "2                       State University of Campinas                 3   \n",
       "3               Federal University of Rio de Janeiro                 4   \n",
       "4                         University of Buenos Aires                 5   \n",
       "..                                               ...               ...   \n",
       "78        Universidade Tecnologica Federal do Parana                79   \n",
       "79                   Universidade Federal de Sergipe                80   \n",
       "80  Universidade Federal Rural de Pernambuco (UFRPE)                81   \n",
       "81           Universidad Autonoma de Baja California                82   \n",
       "82                           Universidad Veracruzana                82   \n",
       "\n",
       "   GlobalScore    Country                             City  PageNumber  \\\n",
       "0         66.4     Brazil                        São Paulo           1   \n",
       "1         57.2      Chile                         Santiago           1   \n",
       "2         56.7     Brazil              Campinas, São Paulo           1   \n",
       "3         54.9     Brazil                   Rio de Janeiro           1   \n",
       "4         53.7  Argentina  Buenos Aires City, Buenos Aires           1   \n",
       "..         ...        ...                              ...         ...   \n",
       "78        16.5     Brazil                         Curitiba           8   \n",
       "79        16.3     Brazil                    São Cristóvão           8   \n",
       "80        15.9     Brazil                       Recife, PE           9   \n",
       "81        15.6     Mexico        Mexicali, Baja California           9   \n",
       "82        15.6     Mexico                 Xalapa, Veracruz           9   \n",
       "\n",
       "                                              USN_URL  \n",
       "0   https://www.usnews.com/education/best-global-u...  \n",
       "1   https://www.usnews.com/education/best-global-u...  \n",
       "2   https://www.usnews.com/education/best-global-u...  \n",
       "3   https://www.usnews.com/education/best-global-u...  \n",
       "4   https://www.usnews.com/education/best-global-u...  \n",
       "..                                                ...  \n",
       "78  https://www.usnews.com/education/best-global-u...  \n",
       "79  https://www.usnews.com/education/best-global-u...  \n",
       "80  https://www.usnews.com/education/best-global-u...  \n",
       "81  https://www.usnews.com/education/best-global-u...  \n",
       "82  https://www.usnews.com/education/best-global-u...  \n",
       "\n",
       "[83 rows x 7 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize fields\n",
    "univurl = []\n",
    "\n",
    "# Defining extra strings in URL and setting up request\n",
    "myurl = 'https://www.usnews.com/education/best-global-universities/latin-america'\n",
    "numpages = 10\n",
    "extrastring = ['']\n",
    "for mypages in range(2,numpages+1):\n",
    "    mytext = '?page='\n",
    "    extrastring.append(mytext+str(mypages))\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "          +' Chrome/64.0.3282.186 Safari/537.36','Accept': 'application/json, text/javascript, */*; q=0.01'}\n",
    "\n",
    "# Loop for pages 1 to numpages\n",
    "for n in range(1,numpages+1):\n",
    "    url = myurl + extrastring[n-1]\n",
    "    source = requests.get(url,headers=header).text\n",
    "    mysoup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "    ### University URLs\n",
    "    for myUname in mysoup.find_all(\"h2\",class_=\"h-taut\"): # All tags with university name\n",
    "        univurl.append(myUname.a['href'])\n",
    "# Append URL list to dataframe\n",
    "tmp = pd.DataFrame({'USN_URL':univurl[0:len(globalscores)]})\n",
    "USN_df = USN_df.join(tmp)\n",
    "USN_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the URL retrieved, the data used to score the universities is available. The following code retrieves such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr><hr></hr><hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>mytable</code> has the table body (in the webpage there is only one table visible). The data in each row is defined in the following way:\n",
    "<ol>\n",
    "    <li>The first column, the ranking position, is a <code>span</code> with <code>class=\"positionInRankin\"</code></li>\n",
    "    <li>The following columns have (in that order) the name of the university between <code>strong</code> tags, location (city), teaching quality, research, prestige, postgraduate offer, internationalization, accreditation, inclusion and diversity and, finally, Quality Index (year). All of them have the same attributes in their respective <code>a<code> tags.</li>\n",
    "    <li>There is an underlying table with some details (accessed by clicking on the corresponding row).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea will be to extract the first (visible) table and then the underlying table. After checking the webpage code (you can try a <code>print(mytable)</code> at the end of the previous code cell), both of them can be accessed at the same level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from the visible table\n",
    "mydf = pd.DataFrame(columns = ['Rank','Name','Location','TeachingQuality','Research','Prestige',\n",
    "                               'PostgraduateOffer','Internationalization','Accrediation','InclusionAndDiversity',\n",
    "                                'QualityIndex2020'])\n",
    "\n",
    "for mytr in mytable.find_all('tr',class_='dataRow'): # Looping for each dataRow in the table\n",
    "    # Initialize data list (row)\n",
    "    data = []\n",
    "    for mycell in mytr.find_all('td'): # Looping for each cell in the row\n",
    "        data.append(mycell.text.strip()) # Strip removes the \\n in the end of the cell data\n",
    "    # Write values from the row\n",
    "    size = len(mydf) # Current size of dataframe\n",
    "    mydf.loc[size] = data # Appending data after last row of dataframe\n",
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from the underlying table\n",
    "myunderdf = pd.DataFrame(columns = ['Name','RK19','TotalProfessors','%FullTimeProfessors','SNIResearchers','PNPCPhDs','PNPCMasters','ExchangeStudents',\n",
    "                                    'TotalBachelorPrograms','%WomenInSrManagement','OwnNativeSupportProgram'])\n",
    "\n",
    "for mytr in mytable.find_all('tr',class_='extraDataRow'): # Looping for each extraDataRow in the table\n",
    "    # Initialize data list (row)\n",
    "    minititle = mytr.find('h3',class_='extraTitle').text.strip() # University's name\n",
    "    data = [minititle]\n",
    "    for mycell in mytr.find_all('span',class_=\"extraMiniValue\"): # Looping for each cell in the row\n",
    "        data.append(mycell.text.strip()) # Strip removes the \\n in the end of the cell data\n",
    "    # Write values from the row\n",
    "    size = len(myunderdf) # Current size of dataframe\n",
    "    myunderdf.loc[size] = data # Appending data after last row of dataframe\n",
    "\n",
    "myunderdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to manage only one dataframe, let's merge the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = mydf.merge(myunderdf,on='Name')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a source with consistent ranking displays (tables) it would be a matter to define a list with the countries and years to retrieve information by looping over the elements of these lists. However, upon further inspection of the source, different countries and different years have different contents and formats in their tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr><hr></hr><hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>0.2 Getting coordinates for the neighborhoods from Problem 2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file to dataframe\n",
    "toronto_df = pd.read_csv('toronto_data.csv')\n",
    "# Changing the name of the first column of downloaded data\n",
    "toronto_df.rename(columns={'Postal Code':'PostalCode'},inplace=True)\n",
    "# Merging provided data into the original dataframe\n",
    "# dataframe is the original data retrieved and cleaned from wikipedia\n",
    "# toronto_df is the downloaded data\n",
    "full_df = pd.merge(dataframe, toronto_df, on='PostalCode')\n",
    "full_df.drop_duplicates(inplace=True) # Dropping duplicated rows\n",
    "print('Shape of merged dataframe: ',merged.shape)\n",
    "full_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.1 Analyzing neighborhoods in Toronto</h4>\n",
    "The purpose of the following code is to group (cluster) different neighborhoods from Toronto in order to see how similar are some of them, and which type of facilities (venues) they have. Maybe you would like to visit neighborhoods with coffee shops and bars one day, and visit neighborhoods with malls and beauty shops another day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get required packages and libraries ready\n",
    "\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "# import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "!conda install -c conda-forge geopy --yes\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "# import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "!conda install -c conda-forge folium=0.5.0 --yes\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2 A first look on Toronto</h4>\n",
    "Let's get some characteristics of the dataframe we have, as well as the location of Toronto in a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many boroughs and neighborhoods does Toronto have?\n",
    "print('The dataframe \"full_df\" for Toronto has {} boroughs and {} neighborhoods.'\n",
    "      .format(len(full_df['Borough'].unique()),\n",
    "              full_df.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is Toronto?\n",
    "address = 'Toronto, Ontario'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"TO_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geographical coordinates of Toronto are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map of Toronto using its latitude and longitude values\n",
    "map_toronto = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# Add markers of neighborhoods to map\n",
    "for lat, lng, borough, neighborhood, pcode in zip(full_df['Latitude'], full_df['Longitude'],\n",
    "                                                  full_df['Borough'], full_df['Neighborhood'],\n",
    "                                                  full_df['PostalCode']):\n",
    "    label = '{} ({}) {}'.format(neighborhood, borough, pcode)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_toronto) # Do not forget to add CircleMarker to the map!!  \n",
    "    \n",
    "map_toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the analysis, the exercise suggests to perform it only in boroughs that include 'Toronto' in its name. Let's extract that information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe by appending the desired boroughs\n",
    "tmp = []\n",
    "for i,x in enumerate(full_df['Borough']): # Create an enumerated list of boroughs\n",
    "    if 'Toronto' in x: # Check if Toronto appears in the borough's name\n",
    "        tmp.append(full_df.iloc[i])\n",
    "\n",
    "justtoronto_df = pd.DataFrame(tmp).reset_index(drop=True) # Transform result to dataframe\n",
    "print('Shape of dataframe for Toronto boroughs: ',justtoronto_df.shape)\n",
    "justtoronto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's adapt the map to the Toronto zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just changed \"full_df\" to \"justtoronto_df\"\n",
    "# And I will overwrite the previous map\n",
    "# Create map of Toronto using its latitude and longitude values\n",
    "map_toronto = folium.Map(location=[latitude, longitude], zoom_start=11) # Larger zoom\n",
    "\n",
    "# Add markers of neighborhoods to map\n",
    "for lat, lng, borough, neighborhood, pcode in zip(justtoronto_df['Latitude'], justtoronto_df['Longitude'],\n",
    "                                                  justtoronto_df['Borough'], justtoronto_df['Neighborhood'],\n",
    "                                                  justtoronto_df['PostalCode']):\n",
    "    label = '{} ({}) {}'.format(neighborhood, borough, pcode)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_toronto) # Do not forget to add CircleMarker to the map!!  \n",
    "    \n",
    "map_toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.1 Setting up Foursquare credentials</h4>\n",
    "Please don't eat up my calls credit! XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'RRYOHBWLN3VNML1RBPM0TRVDW2R41TKNWMZSH0VTOQKGNO2T' # your Foursquare ID\n",
    "CLIENT_SECRET = 'X22FCK21ZCS0UVXZ11TILJFRGXGWVMD5ZADQLIOSMDHHSHHN' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.2 Exploring one neighborhood</h4>\n",
    "In order to make things clear, let's establish the analysis plan using just one neighborhood. Choose by setting a number between 0 and 38 in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up neighborhood to be analyzed\n",
    "nnum = 5\n",
    "\n",
    "myneigh = justtoronto_df.loc[nnum, 'Neighborhood']\n",
    "myneigh_lat = justtoronto_df.loc[nnum, 'Latitude'] # neighborhood latitude value\n",
    "myneigh_lon = justtoronto_df.loc[nnum, 'Longitude'] # neighborhood longitude value\n",
    "\n",
    "print('Your selected neighborhood is {}, located at (latitude,longitude) = ({},{}).'\n",
    "      .format(myneigh, myneigh_lat, myneigh_lon))\n",
    "print('Don\\'t forget to update this cell when you want to analyze other neighborhood!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code requests the top 100 venues in 500 meters around the location of your neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT=100 # Remember the number and type of calls you have in your credit\n",
    "radius=500 # in meters\n",
    "# The URL structure is straighforward to read.\n",
    "# Just remember the information you have to provide for each type of request.\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "           CLIENT_ID,\n",
    "           CLIENT_SECRET,\n",
    "           VERSION,\n",
    "           myneigh_lat,\n",
    "           myneigh_lon,\n",
    "           radius,\n",
    "           LIMIT)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call to Foursquare. Do not abuse of this cell execution!!!\n",
    "results = requests.get(url).json()\n",
    "### results # Careful. Long result ahead. Uncomment just to be sure that it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the information is in the <i>items</i> key. The following function <code>get_category_type</code> is used to extract the name of a category (remember the structure of the information in the <code>json</code> files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function helps to clean the data from the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting \"items\" to work with a smaller amount of data\n",
    "venues = results['response']['groups'][0]['items']\n",
    "\n",
    "# Convert JSON-style data into a table\n",
    "nearby_venues = json_normalize(venues)\n",
    "\n",
    "# Getting only the columns we will use\n",
    "# The names come by looking at the json_normalize result\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues = nearby_venues.loc[:, filtered_columns] # All rows, only the filtered columns\n",
    "\n",
    "# venue.categories looks messy from the previous result. This is why you apply \"get_category_type\"\n",
    "#   to that column, then you get the cleaned name. Of course, the function's design comes after\n",
    "#   checking the data structure in \"venues\".\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# Remove the \"venues.\" string from the column names\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "# Check the result\n",
    "print('{} venues were returned by Foursquare in {}.'.format(nearby_venues.shape[0],myneigh))\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.1 Exploring the full zone</h4>\n",
    "Now that it has been done for one neighborhood, it can be taken to explore the full set of neghborhoods in the selected region of Toronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will do the previous steps with a list of neighborhoods, provided the names and coordinates for each one (and maybe the radius to look for around the location and the limit of venues to search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=500, LIMIT=100):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print('Searching for venues in ',name,'...')\n",
    "            \n",
    "        # Create URL for API request\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # Make GET request, directly retrieving only the interesting part\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # Return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results]) # This is a \"list comprehension\"\n",
    "        # In this type of list, you include an implicit for, which can be useful to reduce the number of lines\n",
    "        #   in a code. In this case, it looks in the \"results\" data for the specific elements and values of the\n",
    "        #   previously defined lists.\n",
    "\n",
    "    # Transform result in dataframe\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list]) # Nested list comprehension\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    print()\n",
    "    print('Done!!',end='\\n\\n')\n",
    "    print('Returned a dataframe with shape ',nearby_venues.shape)\n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the function to the full set of neighborhoods in Toronto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_venues = getNearbyVenues(names=justtoronto_df['Neighborhood'],\n",
    "                                   latitudes=justtoronto_df['Latitude'],\n",
    "                                   longitudes=justtoronto_df['Longitude']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_venues.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many venues does each neighborhood has?\n",
    "print('Number of venues retrieved per neighborhood (dataframe):')\n",
    "toronto_venues.groupby('Neighborhood').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the number of venues returned by Foursquare here matches the one in your \"one neighborhood\" analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many type of venues are there in this dataframe?\n",
    "print('There are {} uniques categories of venues in the dataframe.'.format(len(toronto_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2 Managing the information</h4>\n",
    "The following code will create a dataframe that show how many venues of a given type exists in each neighborhood. The dataframe will be large but this is the preparation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "# Create a dummy dataframe with columns after (unique) values in 'Venue Category'\n",
    "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# Add neighborhood column back to dataframe\n",
    "# With this you just create a 'Neighborhood' column in toronto_onehot\n",
    "#   with the info from toronto_venues['Neighborhood']\n",
    "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
    "\n",
    "# Move neighborhood column to the first column\n",
    "# The previous code results in an alphabetical order in the columns (left-to-right)\n",
    "#   thus let's move the 'Neighborhood' column to the beginning.\n",
    "colind = toronto_onehot.columns.get_loc(\"Neighborhood\") # Getting the position of column in dataframe\n",
    "fixed_columns = [toronto_onehot.columns[colind]] + list(toronto_onehot.columns[0:colind]) + list(toronto_onehot.columns[colind+1:])\n",
    "toronto_onehot = toronto_onehot[fixed_columns]\n",
    "\n",
    "### Warning! In the lab exercise, the 'Neighborhood' column was added at the end of\n",
    "###   the dataframe. That is why there you see a '-1' index to refer to that column.\n",
    "###       fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
    "###       toronto_onehot = toronto_onehot[fixed_columns]\n",
    "###   While checking here, I realized the alphabetical order (don't know why!).\n",
    "###   Thus, I had to modify the code to look for the column by name.\n",
    "\n",
    "toronto_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous dataframe establishes the occurrence of a given venue in a particular neighborhood. Let's group the occurrence of each type (category) of venue per neighborhood, making a <code>mean</code> out of the location to have an idea of the frequency of such occurrence per neighborhood. This is, of the total of venues in a given neighborhood, how feasible is to find a given type of venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "toronto_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the previous results, it is more feasible to find a coffee shop than an art gallery in Berczy Park. This is more easily seen if you print the top 5 venues (according to frequency) for each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for hood in toronto_grouped['Neighborhood']:\n",
    "    print(\"----\" + hood + \"----\") # \"plus\" signs do not work if you mix strings and numbers!\n",
    "    # T is for Transposed. It gets the venue categories to the index side.\n",
    "    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 3})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: Remember that this <i>frequency</i> analysis depends on the number of venues in the neighborhood. If you see very small numbers in the top 5, it may mean there is a lot of venues in the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this information into a dataframe, it is easier to create a function to return the top venues in a . The next cell will create the dataframe in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd'] # Not needed if you use \"Venue #X\" for X = 1 to num_top_venues\n",
    "\n",
    "# Create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind])) # Indicators for 1, 2 and 3\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1)) # When you run out of \"indicators\"\n",
    "\n",
    "# Create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns) # As wide as num_top_venues + 1\n",
    "neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood'] # Copy neighborhoods from dataframe\n",
    "\n",
    "for ind in np.arange(toronto_grouped.shape[0]): # For the number of neighborhoods in the dataframe...\n",
    "    # The function returns the first \"num_top_venues\" from the ordered list from each row\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.1 Clustering neighborhoods using <i>K means</i></h4>\n",
    "The following code runs the <code>K means</code> model on several values for number of clusters and random-number-generator seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
    "for kclusters in range(3,6):\n",
    "    print()\n",
    "    print('Results for K-means with k = ',kclusters)\n",
    "    for seed in range(0,5):\n",
    "        # Execute k-means clustering for given conditions\n",
    "        kmeans = KMeans(n_clusters=kclusters, random_state=seed, n_init=12).fit(toronto_grouped_clustering)    \n",
    "        # Check cluster labels generated for each row in the dataframe\n",
    "        print('For k = {} and seed = {} the labels are: \\n {}'.format(kclusters,seed,kmeans.labels_[0:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the <code>seed</code> for the random number generator that initializes the centroids of the clusters seems to influence more for lower <code>kcluster</code> values. With <code>kclusters=5</code> the results are the same. Let's use those values for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=12).fit(toronto_grouped_clustering)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete the dataframe for Toronto neighborhoods with the data from the neighborhoods, cluster label and top venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clustering labels to the sorted neighborhood venues\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original dataframe (in this case, \"justtoronto_df\")\n",
    "toronto_merged = justtoronto_df\n",
    "\n",
    "# Add neighborhoods_venues_sorted to toronto_merged according to the neighborhood name\n",
    "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "toronto_merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final presentation, a map with colored markers for each cluster is shown as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Toronto's coordinates\n",
    "address = 'Toronto, Ontario'\n",
    "geolocator = Nominatim(user_agent=\"TO_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map object\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# Set color scheme for each cluster\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.gnuplot(np.linspace(0, 1, len(ys))) # Look for color maps in matplotlib\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# Add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, hood, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'],\n",
    "                                  toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(hood) + ' (in Cluster ' + str(cluster) + ')', parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.2 Examining clusters</h4>\n",
    "Why that many neighborhoods are in a specific cluster? Let's see the top venues in each cluster and compare between them. Since cluster 3 is the more populated, let's check that one first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycluster = 0\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == mycluster, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE</b>: If you run this notebook again, the \"big\" cluster can get another label. In this example, it came to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cluster 0, coffee shops and cafés are the common venues on the top list. What happens with neighborhoods like \"Dufferin, Dovercourt Village\" (index 9)? It does not seem very similar. It shares bakery and bar on his top venues with a couple of other neighborhoods but it seems rather odd. Maybe the analysis tends to load the separation on the top venues rather than the whole set. Anyway, remember we are looking at the top venues here, not at every one of them. For the rest of the clusters, the comparison is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycluster = 1\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == mycluster, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycluster = 2\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == mycluster, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycluster = 3\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == mycluster, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycluster = 4\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == mycluster, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the clusters with more than one element, top venues are very similar. There, the clustering makes sense. It may be a challenge to further analyze the data in order to see why the clustering puts that many neighborhoods in one of them (remember the results for <code>kclusters</code> from 3 to 4 in the beginning of section 4.1). Some straightforward ideas on this can be found <a href=\"https://zerowithdot.com/mistakes-with-k-means-clustering/\">here</a> and some solutions are suggested <a href=\"https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/\">here</a>. Since this is a high-dimensionality problem, the suggestion I have is to try several clusters and check the label distribution. Just set <code>maxclusters</code> in the following cell and see what's a good candidate! After that, rinse and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
    "maxclusters = 10\n",
    "seed = 0\n",
    "save_k = 10\n",
    "for kclusters in range(3,maxclusters+1):\n",
    "    print()\n",
    "    print('Results for K-means with k = ',kclusters)\n",
    "    # Execute k-means clustering for given conditions\n",
    "    tmp = KMeans(n_clusters=kclusters, random_state=seed, n_init=12).fit(toronto_grouped_clustering)    \n",
    "    # Check cluster labels generated for each row in the dataframe\n",
    "    print('For k = {} and seed = {} the labels are: \\n {}'.format(kclusters,seed,tmp.labels_[0:]))\n",
    "    if kclusters == save_k:\n",
    "        kmeans = tmp\n",
    "print()\n",
    "print('Saved results for kclusters = ',save_k,' in \"kmeans\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
